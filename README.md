# Comparing Two Human-Computer Interactive Textual Analysis to Identify Evidence for Policymaking: Analyzing Interview Data for Advancing Educational Equity

## Abstract
Obtaining stakeholders’ diverse experiences and opinions about current policy on a timely manner at scale is crucial for policymakers to identify assets and gaps in resource allocations to support policy design and implementation. However, manually coding even a moderate amount of interview texts or open-ended survey data from stakeholders can often be labor intensive and time costly. Although automated text analysis has promise to reduce these costs, policymakers may be less inclined to completely rely on an automated approach that is not based on disciplinary theories and policy contexts. Integrating human experts’ inputs into automatic textual analysis may mediate policymakers’ concerns. In this study, we compare two human-computer interactive learning approaches to analyze stakeholders’ interviews about policies that advance racial and economic equity in K-12 public schools in one U.S. state. With *computer-human parallel* analysis, human coding guided by a domain-specific theoretical framework happens in parallel with unsupervised topic modeling. In the second *computer-human sequential* approach, unsupervised topic modeling occurs first and then human coders use the initial theme discovery to develop a codebook and then code the interview data. While each approach offers nuances and allows human experts to use their domain knowledge to validate, interpret, and supplement the computer analysis results, the *computer-human sequential* approach offers a better integration of the advantages of both computer and human coding to enable a faster evidence generation to support policy decision making in this study. 
